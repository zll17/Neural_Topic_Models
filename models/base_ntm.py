#!/usr/bin/env python
# -*- encoding: utf-8 -*-

import os
import time
import numpy as np
import torch
from torch.utils.data import DataLoader
from utils import *

class BaseNTM(object):
    def __init__(self, name: str):
        self.name = name
        self.dict_path = None
        self.param = {}
        self.state = {}
        self.save_dir = None

    def __getitem__(self, bow):
        return self.get_document_topics(bow)

    def train(self):
        pass
    
    # def update(self):
    #     ''' 
    #     Update (train) model incrementally with unseen documents.
    #     Adapted from gensim.
    #     Add or not?
    #     '''
    #     pass

    # def get_embed(self):
    #     pass
    
    # def get_topic_word_dist(self):
    #     ''' 
    #     gensim: __getitem__
    #     '''
    #     pass
    
    # def show_topic_words(self):
    #     '''
    #     gensim: __getitem__
    #     '''
    #     pass
 
    def evaluate(self):
        pass
    
    # def diff(self):
    #     '''
    #     gensim: Calculate the difference in topic distributions between two models: self and other.
    #     Add or not?
    #     '''
    #     pass

    def inference(self):
        pass
    
    def get_document_topics(self, bow, minimum_probability=None):
        '''
        :param bow: list in the format of [(token_idx, freq), (token_idx, freq), ...]
        :return: list of (topic id, probability)
        '''
        if bow==[] or bow is None:
            return []
        doc_bow = expand_bow(bow, self.bow_dim)  # gensim doc bow List format -> torch tensor
        topics = self.inference(doc_bow)  # numpy array dim=(1,num_topics)
        topics_sorted = sort_topics(topics)[0]
        return topics_sorted

    def inference_dataset(self, train_data, num=-1, batch_size=512, shuffle=False, num_workers=4):
        # self.vae.eval()
        data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=train_data.collate_fn)
        embed_lst = []
        txt_lst = []
        if num==-1:
            num=len(train_data)
        cnt = 0
        for data_batch in data_loader:
            txts, bows = data_batch
            embed = self.inference(bows)
            embed_lst.append(embed)
            txt_lst.append(txts)
            cnt += embed.shape[0]
            if cnt>=num:
                break
        embed_lst = np.concatenate(embed_lst,axis=0)[:num]
        txt_lst = np.concatenate(txt_lst,axis=0)[:num]
        return txt_lst, embed_lst

    def _get_topics(self, num_words):
        pass

    def get_topic_terms(self, topicid, topn=10):
        '''Get the representation for a single topic. Words the integer IDs, in constrast to show_topic() that represents words by the actual strings.
        :return: Word ID - probability pairs for the most relevant words generated by the topic.
        :return type: list of (int, float)
        '''
        indices, vals = self._get_topics(num_words=topn)
        topic_words=[(indices[topicid][i], vals[topicid][i]) for i in range(topn)]
        return topic_words

    def show_topics(self, num_topics=-1, num_words=15, dictionary=None, show_val=False):
        ''' Get the most significant topics
        :param num_topics: default to -1 to print all topics
        :param num_words: show how many words per topic
        :param dictionary: set dictionary if self.id2token is None [to remove]
        :param show_val: whether to show probability of words
        :return: sequence with (topic_id, [(word, value), … ]).
        :return type: list of (int, list of (str, float))
        Note: the subclass must has self.id2token and self.n_topics to call this method
        '''
        topic_words = []
        indices, vals = self._get_topics(num_words=num_words)
        if self.id2token==None and dictionary!=None:
            self.id2token = {v:k for k,v in dictionary.token2id.items()}
        if show_val:
            for i in range(min(num_topics, self.n_topic) if num_topics>0 else self.n_topic):
                topic_words.append([(self.id2token[indices[i][j]], vals[i][j]) for j in range(num_words)])
        else:
            for i in range(min(num_topics, self.n_topic) if num_topics>0 else self.n_topic):
                topic_words.append([self.id2token[indices[i][j]] for j in range(num_words)])
        return [(i, topic) for i, topic in enumerate(topic_words)]

    def print_topics(self, num_topics=-1, num_words=15, dictionary=None, show_val=True):
        ''' Get the most significant topics (alias for show_topics() method).
        :return: Sequence with (topic_id, [(word, value), … ]).
        '''
        l = self.show_topics(num_topics, num_words, dictionary, show_val)
        for (i, topic_words) in l:
            print("topic{}: {}".format(i,str(topic_words)))

    def show_topic(self, topicno, topn=10, dictionary=None, show_val=True):
        ''' Get a single topic as a formatted string.
        :param topicno: (int) Topic id.
        :prarm topn: (int) Number of words from topic that will be used.
        :param dictionary: set dictionary if self.id2token is None [to remove]
        :param show_val: whether to show probability of words.
        :return: (topic_id, [(word, value), … ])
        :return type: list of (str, float)
        '''
        l = self.show_topics(-1, topn, dictionary, show_val)
        return l[topicno][1]

    def print_topic(self, topicno, topn=10, dictionary=None, show_val=True):
        ''' Get a single topic as a formatted string.
        :return: String representation of topic, like ‘-0.340 * “category” + 0.298 * “$M$” + 0.183 * “algebra” + … ‘.
        '''
        topic_words = self.show_topic(topicno, topn, dictionary, show_val)
        if show_val:
            print("topic{}: ".format(topicno) + "+".join(["{}*\"{}\"".format(str(val),word) for (word, val) in topic_words]))
        else:
            print("topic{}: {}".format(topicno,str(topic_words)))

    def _update_param(self, **kwargs):
        self.param.update(kwargs)

    def _update_state(self, **kwargs):
        self.state.update(kwargs)

    def save(self, save_path: str = None):
        '''
            If called by user, need to specify the save_path.
            If called by subclass, save_path is set by default.
        '''
        if save_path is None:
            if self.save_dir is None:
                folder_name = f'{time.strftime("%m-%d-%H-%M", time.localtime())}_{self.name}_tp{self.param["n_topic"]}'
                self.save_dir = os.path.join(os.getcwd(),'ckpt',folder_name)
            if not os.path.exists(self.save_dir):
                os.mkdir(self.save_dir)
            save_path = os.path.join(self.save_dir, "ep%d.ckpt"%self.state["epoch"])

        ckpt = {"state": self.state, "param": self.param}
        torch.save(ckpt, save_path)
        print("Checkpoint saved to %s"%save_path)