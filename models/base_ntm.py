#!/usr/bin/env python
# -*- encoding: utf-8 -*-

import os
import time
import numpy as np
import torch
from torch.utils.data import DataLoader
from utils import *

class BaseNTM(object):
    def __init__(self, name: str):
        self.name = name
        self.dict_path = None
        self.param = {}
        self.state = {}
        self.save_dir = None

    def __getitem__(self, bow):
        return self.get_document_topics(bow)

    def train(self):
        pass
    
    # def update(self):
    #     ''' 
    #     Update (train) model incrementally with unseen documents.
    #     Adapted from gensim.
    #     Add or not?
    #     '''
    #     pass
    
    def evaluate(self):
        pass
    
    # def diff(self):
    #     '''
    #     gensim: Calculate the difference in topic distributions between two models: self and other.
    #     Add or not?
    #     '''
    #     pass

    def inference(self):
        pass
    
    def get_document_topics(self, bow, minimum_probability=None):
        '''Inference for one document
        :param bow: list in the format of [(token_idx, freq), (token_idx, freq), ...]
        :return: list of (topic id, probability)
        '''
        if bow==[] or bow is None:
            return []
        doc_bow = expand_bow(bow, self.bow_dim)  # gensim doc bow List format -> torch tensor
        topics = self.inference(doc_bow)  # numpy array dim=(1,num_topics)
        topics_sorted = sort_topics(topics, min_prob=minimum_probability)[0]
        return topics_sorted

    def inference_dataset(self, train_data, num=-1, batch_size=512, shuffle=False, num_workers=4):
        # self.vae.eval()
        data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=train_data.collate_fn)
        embed_lst = []
        txt_lst = []
        if num==-1:
            num=len(train_data)
        cnt = 0
        for data_batch in data_loader:
            txts, bows = data_batch
            embed = self.inference(bows)
            embed_lst.append(embed)
            txt_lst.append(txts)
            cnt += embed.shape[0]
            if cnt>=num:
                break
        embed_lst = np.concatenate(embed_lst,axis=0)[:num]
        txt_lst = np.concatenate(txt_lst,axis=0)[:num]
        return txt_lst, embed_lst

    def get_topic_word_dist(self):
        '''Get topic word distribution, the weight of each words for all topics
        :return: numpy array in shape (num topics, vocab size)
        '''
        pass

    def get_topic_terms(self, topicid, topn=10):
        '''Get the representation for a single topic. Words the integer IDs, in constrast to show_topic() that represents words by the actual strings.
        :return: Word ID - probability pairs for the most relevant words generated by the topic.
        :return type: list of (int, float)
        '''
        word_dist = self.get_topic_word_dist()  # 2-d numpy array
        topic_terms = sort_topics(word_dist[topicid], topk=topn)[0]  # list of [(token id, prob),...]
        return topic_terms

    def show_topics(self, num_topics=-1, num_words=15, dictionary=None, show_val=False):
        '''Print topic - word representation of a model. Topics are ordered by index, not sorted.
        :param num_topics: default to -1 to print all topics.
        :param num_words: show how many words per topic
        :param dictionary: set dictionary if self.id2token is None [TO REMOVE]
        :param show_val: whether to show probability of words
        :return: sequence with (topic_id, [(word, value), … ]).
        :return type: list of (int, list of (str, float))
        Note: the subclass must has self.id2token to call this method
        '''
        word_dist = self.get_topic_word_dist()  # 2-d numpy array
        if num_topics<0:
            num_topics = word_dist.shape[0]
        sorted_word_dist = sort_topics(word_dist[:num_topics], topk=num_words)  # list of [(token id, prob),...]
        if self.id2token==None and dictionary!=None:
            self.id2token = {v:k for k,v in dictionary.token2id.items()}
        res = []
        for i in range(num_topics):
            if show_val:
                res.append((i,[(self.id2token[token_id], prob) for (token_id, prob) in sorted_word_dist[i]]))
            else:
                res.append((i,[self.id2token[token_id] for (token_id, prob) in sorted_word_dist[i]]))
        return res

    def print_topics(self, num_topics=-1, num_words=15, dictionary=None, show_val=True):
        ''' Get the most significant topics (alias for show_topics() method).
        :return: Sequence with (topic_id, [(word, value), … ]).
        '''
        l = self.show_topics(num_topics, num_words, dictionary, show_val)
        for (i, topic_words) in l:
            print("topic{}: {}".format(i,str(topic_words)))

    def show_topic(self, topicno, topn=10, dictionary=None, show_val=True):
        ''' Get a single topic as a formatted string.
        :param topicno: (int) Topic id.
        :prarm topn: (int) Number of words from topic that will be used.
        :param dictionary: set dictionary if self.id2token is None [to remove]
        :param show_val: whether to show probability of words.
        :return: (topic_id, [(word, value), … ])
        :return type: list of (str, float)
        '''
        l = self.show_topics(-1, topn, dictionary, show_val)
        return l[topicno][1]

    def print_topic(self, topicno, topn=10, dictionary=None, show_val=True):
        ''' Get a single topic as a formatted string.
        :return: String representation of topic, like ‘-0.340 * “category” + 0.298 * “$M$” + 0.183 * “algebra” + … ‘.
        '''
        topic_words = self.show_topic(topicno, topn, dictionary, show_val)
        if show_val:
            print("topic{}: ".format(topicno) + "+".join(["{}*\"{}\"".format(str(val),word) for (word, val) in topic_words]))
        else:
            print("topic{}: {}".format(topicno,str(topic_words)))

    def _update_param(self, **kwargs):
        self.param.update(kwargs)

    def _update_state(self, **kwargs):
        self.state.update(kwargs)

    def save(self, save_path: str = None):
        '''
            If called by user, need to specify the save_path.
            If called by subclass, save_path is set by default.
        '''
        if save_path is None:
            if self.save_dir is None:
                folder_name = f'{time.strftime("%m-%d-%H-%M", time.localtime())}_{self.name}_tp{self.param["n_topic"]}'
                self.save_dir = os.path.join(os.getcwd(),'ckpt',folder_name)
            if not os.path.exists(self.save_dir):
                os.mkdir(self.save_dir)
            save_path = os.path.join(self.save_dir, "ep%d.ckpt"%self.state["epoch"])

        ckpt = {"state": self.state, "param": self.param}
        torch.save(ckpt, save_path)
        print("Checkpoint saved to %s"%save_path)